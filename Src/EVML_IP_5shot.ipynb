{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qhjJVMXmHYem"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mI4ki_ZJFfBz"
   },
   "outputs": [],
   "source": [
    "!pip install libmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bL5Pcw-LUzgu"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import Sequential,layers\n",
    "from tensorflow.keras.layers import Conv3D,BatchNormalization, MaxPool2D, Activation, Flatten, Dense, GlobalAveragePooling3D, GlobalMaxPool3D, AveragePooling3D, Lambda, Reshape, UpSampling2D, Conv2DTranspose \n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import datetime\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from itertools import repeat\n",
    "import matplotlib\n",
    "import libmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2TDRLDzQ7MRv"
   },
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "\n",
    "def applyPCA(X, numComponents=75):\n",
    "    newX = np.reshape(X, (-1, X.shape[2]))\n",
    "    pca = PCA(n_components=numComponents, whiten=True)\n",
    "    newX = pca.fit_transform(newX)\n",
    "    newX = np.reshape(newX, (X.shape[0],X.shape[1], numComponents))\n",
    "    return newX, pca\n",
    "\n",
    "def padWithZeros(X, margin=2):\n",
    "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
    "    x_offset = margin\n",
    "    y_offset = margin\n",
    "    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
    "    return newX\n",
    "\n",
    "def createImageCubes(X, y, windowSize, removeZeroLabels = True):\n",
    "    margin = int((windowSize - 1) / 2)\n",
    "    zeroPaddedX = padWithZeros(X, margin=margin)  # X :(145, 145, 30) --> (195, 195, 30) with window =25\n",
    "    # split patches\n",
    "    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))  # (21025, 25, 25, 30)   \n",
    "    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))  # (21025,)\n",
    "    patchIndex = 0\n",
    "    \n",
    "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
    "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
    "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]  \n",
    "            patchesData[patchIndex, :, :, :] = patch\n",
    "            patchesLabels[patchIndex] = y[r-margin, c-margin]            \n",
    "            patchIndex = patchIndex + 1\n",
    "  \n",
    "    patchesData = np.expand_dims(patchesData, axis=-1)\n",
    "    return patchesData,patchesLabels\n",
    "\n",
    "def patches_class(X,Y,n):\n",
    "    n_classes = n\n",
    "    patches_list = []\n",
    "    labeles_list = []\n",
    "    for i in range(1,n_classes+1):   # not considering class 0\n",
    "        patchesData_Ith_Label = X[Y==i,:,:,:,:]\n",
    "        Ith_Label = Y[Y==i]\n",
    "        patches_list.append(patchesData_Ith_Label)\n",
    "        labeles_list.append(Ith_Label)\n",
    "        \n",
    "    return patches_list,labeles_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2Dnqh5A7MRz"
   },
   "outputs": [],
   "source": [
    "windowSize = 11\n",
    "im_height, im_width, im_depth, im_channel = windowSize, windowSize, 30, 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iMSAOM647MR0"
   },
   "outputs": [],
   "source": [
    "X = sio.loadmat('/content/drive/MyDrive/data/Indian_pines_corrected.mat')['indian_pines_corrected']\n",
    "y = sio.loadmat('/content/drive/MyDrive/data/Indian_pines_gt.mat')['indian_pines_gt']\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "X,pca = applyPCA(X,numComponents=im_depth)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "X, y = createImageCubes(X, y, windowSize)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "patches_class_ip,label_ip = patches_class(X,y,16) # class_wise list of patches #(16,) for class 0: (2009, 9, 9, 20, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bgybQF0fdQqB"
   },
   "outputs": [],
   "source": [
    "# Assign label of coarse classses\n",
    "# -------------------------------\n",
    "# vegetation - 0\n",
    "# Grasslands - 1\n",
    "# woodland - 2\n",
    "# Urban - 3\n",
    "# water - 4\n",
    "\n",
    "# Map True classes to coarse classes\n",
    "# ----------------------------------\n",
    "# True-class     coarse-class   coarse-ID\n",
    "# - - - - - - - - - - - - - - - - - -  -  -\n",
    "#       1\t        vegetation        0\n",
    "#       2\t        vegetation        0  \n",
    "#       3\t        vegetation        0\n",
    "#       4\t        vegetation        0\n",
    "#       5\t        Grasslands        1\n",
    "#       6\t        Grasslands        1\n",
    "#       7\t        Grasslands        1\n",
    "#       8\t        Grasslands        1\n",
    "#       9\t        vegetation        0\n",
    "#      10\t        vegetation        0\n",
    "#      11\t        vegetation        0\n",
    "#      12\t        vegetation        0\n",
    "#      13\t        vegetation        0\n",
    "#      14\t        woodland          2\n",
    "#      15\t          Urban           3\n",
    "#      16\t          Urban           3\n",
    "\n",
    "y_coarse = dict()\n",
    "y_coarse[1] = [1,0,0,0,0]    # ToDo : [1,0,0,0]\n",
    "y_coarse[2] = [1,0,0,0,0]\n",
    "y_coarse[3] = [1,0,0,0,0]\n",
    "y_coarse[4] = [1,0,0,0,0]\n",
    "y_coarse[5] = [0,1,0,0,0]\n",
    "y_coarse[6] = [0,1,0,0,0]\n",
    "y_coarse[7] = [0,1,0,0,0]\n",
    "y_coarse[8] = [0,1,0,0,0]\n",
    "y_coarse[9] = [1,0,0,0,0]\n",
    "y_coarse[10] = [1,0,0,0,0]\n",
    "y_coarse[11] = [1,0,0,0,0]\n",
    "y_coarse[12] = [1,0,0,0,0]\n",
    "y_coarse[13] = [1,0,0,0,0]\n",
    "y_coarse[14] = [0,0,1,0,0]\n",
    "y_coarse[15] = [0,0,0,1,0]\n",
    "y_coarse[16] = [0,0,0,1,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mgzi7xM2vADV"
   },
   "outputs": [],
   "source": [
    "coarse_dict = dict()\n",
    "coarse_dict[1] = 0    # ToDo : [1,0,0,0]\n",
    "coarse_dict[2] = 0\n",
    "coarse_dict[3] = 0\n",
    "coarse_dict[4] = 0\n",
    "coarse_dict[5] = 1\n",
    "coarse_dict[6] = 1\n",
    "coarse_dict[7] = 1\n",
    "coarse_dict[8] = 1\n",
    "coarse_dict[9] = 0\n",
    "coarse_dict[10] = 0\n",
    "coarse_dict[11] = 0\n",
    "coarse_dict[12] = 0\n",
    "coarse_dict[13] = 0\n",
    "coarse_dict[14] = 2\n",
    "coarse_dict[15] = 3\n",
    "coarse_dict[16] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbaGmUqg7MR7"
   },
   "outputs": [],
   "source": [
    "train_class_indices = [1,2,4,5,7,9,10,11,13,14]    # 10 classes  \n",
    "train_class_labels = [2,3,5,6,8,10,11,12,14,15]\n",
    "\n",
    "test_class_indices = [0,3,6,8,12,15]               # 6 classes\n",
    "test_class_labels = [1,4,7,9,13,16]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MH3SyUEz7MR9"
   },
   "outputs": [],
   "source": [
    "# Definition of Episodes\n",
    "\n",
    "# replace=False - no repeat\n",
    "def new_episode(patches_list,NS,NQ,CS,CQ,class_labels) :  # NS 5,NQ 15,CS 3,CQ 6\n",
    "    selected_classes = list(np.random.choice(class_labels,CQ,replace=False))  # Randomly choice 6 Query Classes\n",
    "    support_classes = list(np.random.choice(selected_classes,CS,replace=False))  # Randomly choice 3 Support Classes from Q\n",
    "    \n",
    "    tquery_patches,tsupport_patches = [],[]\n",
    "    query_labels,support_labels = [],[]\n",
    "    \n",
    "    for x in support_classes :      #3\n",
    "        sran_indices = np.random.choice(patches_list[x-1].shape[0],NS,replace=False) # K=5 img nos from 20 per class for Support\n",
    "        support_patches = patches_list[x-1][sran_indices,:,:,:,:]  # for x class those 5 nos\n",
    "        tsupport_patches.extend(support_patches)               # 3 class * 5 patch per class = 15 patches\n",
    "        for i in range(NS) :\n",
    "            support_labels.append(x)                           # 3 class *5 nos = 15 nos\n",
    "        \n",
    "    for x in selected_classes :     #6\n",
    "        qran_indices = np.random.choice(patches_list[x-1].shape[0],NQ,replace=False) # NQ=15 img nos from 20 per class for Query\n",
    "        query_patches = patches_list[x-1][qran_indices,:,:,:,:]    # for x class those 15 nos\n",
    "        tquery_patches.extend(query_patches)                   # 6 class * 15 patch per class = 90 patches\n",
    "        for i in range(NQ) :\n",
    "            query_labels.append(x)                             # 6 class * 15 nos = 90 nos\n",
    "    \n",
    "    temp1 = list(zip(tquery_patches, query_labels)) \n",
    "    random.shuffle(temp1)        # By Doing Shuffling, Support, Query Same class combination got mismatched - mitigated by support index\n",
    "    tquery_patches, query_labels = zip(*temp1)\n",
    "    \n",
    "    tquery_patches = tf.convert_to_tensor(np.reshape(np.asarray(tquery_patches),(CQ*NQ,11,11,30,1)),dtype=tf.float32)\n",
    "    tsupport_patches = tf.convert_to_tensor(np.reshape(np.asarray(tsupport_patches),(CS*NS,11,11,30,1)),dtype=tf.float32)\n",
    "    return tquery_patches, tsupport_patches, query_labels, support_labels, support_classes    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-4DJNBmXh5e"
   },
   "outputs": [],
   "source": [
    "# Coarse Episode definition\n",
    "\n",
    "def new_episode_coarse(patches_list,NQ,CS,CQ,Total_labels,S_Proto_classes) :  # NS 5,NQ 15,CS 3,CQ 6   \n",
    "    U_c = list(np.random.choice( list(set(Total_labels).difference(set(S_Proto_classes))),CQ-CS,replace=False)) #train_class_labels\n",
    "    query_classes = S_Proto_classes.copy()\n",
    "    query_classes.extend(U_c)\n",
    "    \n",
    "    tquery_patches = []\n",
    "    query_labels = []\n",
    "\n",
    "    for x in query_classes :     #6\n",
    "        qran_indices = np.random.choice(patches_list[x-1].shape[0],NQ,replace=False) # NQ=15 img nos from 20 per class for Query\n",
    "        query_patches = patches_list[x-1][qran_indices,:,:,:,:]    # for x class those 15 nos\n",
    "        tquery_patches.extend(query_patches)                   # 6 class * 15 patch per class = 90 patches\n",
    "        for i in range(NQ) :\n",
    "          if x==query_classes[0]:\n",
    "            query_labels.append([1,0,0,0,0])                             # 6 class * 15 nos = 90 nos\n",
    "          elif x==query_classes[1]:\n",
    "            query_labels.append([0,1,0,0,0]) \n",
    "          elif x==query_classes[2]:\n",
    "            query_labels.append([0,0,1,0,0]) \n",
    "          elif x==query_classes[3]:\n",
    "            query_labels.append([0,0,0,1,0])  \n",
    "          else:\n",
    "            query_labels.append([0,0,0,0,1])             \n",
    " \n",
    "    OOD_GT = []\n",
    "    for l in query_classes:\n",
    "      O_class = np.argmax(y_coarse[l]) # convert to correponding outlier class\n",
    "      OOD_GT.extend(repeat(O_class,NQ))\n",
    "\n",
    "    tquery_patches = tf.convert_to_tensor(np.reshape(np.asarray(tquery_patches),(CQ*NQ,11,11,30,1)),dtype=tf.float32)\n",
    "\n",
    "    return tquery_patches, OOD_GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YKeCjjvFbg8"
   },
   "outputs": [],
   "source": [
    "# Quadruplet Loss Definition\n",
    "\n",
    "def calc_Quadruplet_dists(Proto, query, query_labels):  # [3,64], [90,64], [90,3]\n",
    "    \n",
    "    nbprototypes = Proto.shape[0]    # 3\n",
    "    nbqueries = query.shape[0]       # 90\n",
    "    Quadruplet_P_Nk, Quadruplet_P_Nu, Quadruplet_Nk_Nu = [], [], []\n",
    "    Alpha1, Alpha2, Alpha3 = 0.5, 1, 0.5\n",
    "    \n",
    "    for i in range(nbprototypes):\n",
    "        pos_ind = np.where(query_labels[:,i]==1)   # indexes of Positive queries for i th prototype\n",
    "        pos_ind = list(pos_ind[0])            # indexes of Positive queries for i th prototype\n",
    "        Neg_ind = list(set(np.arange(nbqueries)).difference(set(pos_ind)))  # indexes of Negative queries for i th prototype\n",
    "        Neg_ind_U = []\n",
    "        for u in range(nbqueries):\n",
    "          if sum(query_labels[u])==0:\n",
    "            Neg_ind_U.append(u)\n",
    "        Neg_ind_K = list(set(Neg_ind).difference(set(Neg_ind_U)))\n",
    "\n",
    "        Anchor = tf.expand_dims(Proto[i], 0)     # [1, 64]       # ith Prototype / Anchor\n",
    "        \n",
    "        Positive_d = []\n",
    "        for j in range(len(pos_ind)):   # 15\n",
    "            Pos = tf.expand_dims(query[pos_ind[j]], 0)   # [1, 64]\n",
    "            Pos_dist = tf.reduce_mean(tf.math.pow(Anchor-Pos, 2), 1)   # scalar\n",
    "            Positive_d.append(Pos_dist)                    # List\n",
    "        Positive_d = tf.reduce_mean(Positive_d) # scalar\n",
    "        \n",
    "        Negative_d_k = []\n",
    "        for j in range(len(Neg_ind_K)):\n",
    "            Neg = tf.expand_dims(query[Neg_ind_K[j]], 0)   # [1, 64]\n",
    "            Neg_dist = tf.reduce_mean(tf.math.pow(Anchor-Neg, 2), 1)   # scalar\n",
    "            Negative_d_k.append(Neg_dist)                     # List\n",
    "        Negative_d_k = tf.reduce_mean(Negative_d_k) # scalar\n",
    "\n",
    "        Negative_d_U = []\n",
    "        for j in range(len(Neg_ind_U)):\n",
    "            Neg = tf.expand_dims(query[Neg_ind_U[j]], 0)   # [1, 64]\n",
    "            Neg_dist = tf.reduce_mean(tf.math.pow(Anchor-Neg, 2), 1)   # scalar\n",
    "            Negative_d_U.append(Neg_dist)                     # List\n",
    "        Negative_d_U = tf.reduce_mean(Negative_d_U) # scalar\n",
    "        \n",
    "        P_Nk = Positive_d-Negative_d_k\n",
    "        P_Nu = Positive_d-Negative_d_U\n",
    "        Nk_Nu = Negative_d_k-Negative_d_U        \n",
    "        Quadruplet_P_Nk.append(P_Nk)            #list\n",
    "        Quadruplet_P_Nu.append(P_Nu) \n",
    "        Quadruplet_Nk_Nu.append(Nk_Nu) \n",
    "        \n",
    "    P_Nk_Proto_mean = tf.reduce_mean(Quadruplet_P_Nk)    # p_dist - n_dist  # averaged for all Prototypes   # scalar \n",
    "    P_Nu_Proto_mean = tf.reduce_mean(Quadruplet_P_Nu)\n",
    "    Nk_Nu_Proto_mean = tf.reduce_mean(Quadruplet_Nk_Nu)\n",
    "\n",
    "    return tf.math.maximum(P_Nk_Proto_mean + Alpha1, 0)+tf.math.maximum(P_Nu_Proto_mean + Alpha2, 0)+tf.math.maximum(Nk_Nu_Proto_mean + Alpha3, 0)  # scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-zd2ipd7MSI"
   },
   "source": [
    "# Define Model\n",
    "###### CBAM3D Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Svzd5IR2q0C8"
   },
   "outputs": [],
   "source": [
    "class Channel_Attention_3D(tf.keras.layers.Layer) :\n",
    "    def __init__(self,C,ratio) :\n",
    "        super(Channel_Attention_3D,self).__init__()\n",
    "        self.avg_pool = GlobalAveragePooling3D()\n",
    "        self.max_pool = GlobalMaxPool3D()\n",
    "        self.activation = Activation('sigmoid')\n",
    "        self.fc1 = Dense(C/ratio, activation = 'relu')\n",
    "        self.fc2 = Dense(C)    \n",
    "    def call(self,x) :\n",
    "        avg_out1 = self.avg_pool(x)\n",
    "        avg_out2 = self.fc1(avg_out1)\n",
    "        avg_out3 = self.fc2(avg_out2)\n",
    "        max_out1 = self.max_pool(x)\n",
    "        max_out2 = self.fc1(max_out1)\n",
    "        max_out3 = self.fc2(max_out2)\n",
    "        add_out = tf.math.add(max_out3,avg_out3)\n",
    "        channel_att = self.activation(add_out)\n",
    "        return channel_att \n",
    "    \n",
    "class Spatial_Attention_3D(tf.keras.layers.Layer) :\n",
    "    def __init__(self) :\n",
    "        super(Spatial_Attention_3D,self).__init__()\n",
    "        self.conv3d = Conv3D(1,(7,7,7),padding='same',activation='sigmoid')\n",
    "        self.avg_pool_chl = Lambda(lambda x:tf.keras.backend.mean(x,axis=4,keepdims=True))\n",
    "        self.max_pool_chl = Lambda(lambda x:tf.keras.backend.max(x,axis=4,keepdims=True)) \n",
    "    def call(self,x) :\n",
    "        avg_out1 = self.avg_pool_chl(x)\n",
    "        max_out1 = self.max_pool_chl(x)\n",
    "        concat_out = tf.concat([avg_out1,max_out1],axis=-1)\n",
    "        spatial_att = self.conv3d(concat_out)\n",
    "        return spatial_att \n",
    "    \n",
    "class CBAM_3D(tf.keras.layers.Layer) :\n",
    "    def __init__(self,C,ratio) :\n",
    "        super(CBAM_3D,self).__init__()\n",
    "        self.C = C\n",
    "        self.ratio = ratio\n",
    "        self.channel_attention = Channel_Attention_3D(self.C,self.ratio)\n",
    "        self.spatial_attention = Spatial_Attention_3D()\n",
    "    def call(self,y,H,W,D,C) :\n",
    "        ch_out1 = self.channel_attention(y)\n",
    "        ch_out2 = tf.expand_dims(ch_out1, axis=1)\n",
    "        ch_out3 = tf.expand_dims(ch_out2, axis=2)\n",
    "        ch_out4 = tf.expand_dims(ch_out3, axis=3)\n",
    "        ch_out5 = tf.tile(ch_out4, multiples=[1,H,W,D,1])\n",
    "        ch_out5 = tf.math.multiply(ch_out5,y)\n",
    "        sp_out1 = self.spatial_attention(ch_out5)\n",
    "        sp_out2 = tf.tile(sp_out1, multiples = [1,1,1,1,C])\n",
    "        sp_out3 = tf.math.multiply(sp_out2,ch_out5)\n",
    "        return sp_out3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cedf6y6w7MSK"
   },
   "source": [
    "### R3CBAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VKHKtyU47MSK",
    "outputId": "5336d771-8285-42f5-cdbc-63605226fe37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"R3CBAM\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 11, 11, 30,  0           []                               \n",
      "                                 1)]                                                              \n",
      "                                                                                                  \n",
      " conv3d (Conv3D)                (None, 11, 11, 30,   224         ['input_2[0][0]']                \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv3d_2 (Conv3D)              (None, 11, 11, 30,   1736        ['conv3d[0][0]']                 \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " cbam_3d_1 (CBAM_3D)            (None, 11, 11, 30,   729         ['conv3d_2[0][0]']               \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv3d_4 (Conv3D)              (None, 11, 11, 30,   1736        ['cbam_3d_1[0][0]']              \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 11, 11, 30,   0           ['conv3d[0][0]',                 \n",
      "                                8)                                'conv3d_4[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling3d (MaxPooling3D)   (None, 6, 6, 8, 8)   0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv3d_5 (Conv3D)              (None, 6, 6, 8, 16)  3472        ['max_pooling3d[0][0]']          \n",
      "                                                                                                  \n",
      " cbam_3d_2 (CBAM_3D)            (None, 6, 6, 8, 16)  835         ['conv3d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv3d_7 (Conv3D)              (None, 6, 6, 8, 16)  6928        ['cbam_3d_2[0][0]']              \n",
      "                                                                                                  \n",
      " cbam_3d_3 (CBAM_3D)            (None, 6, 6, 8, 16)  835         ['conv3d_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv3d_9 (Conv3D)              (None, 6, 6, 8, 16)  6928        ['cbam_3d_3[0][0]']              \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 6, 6, 8, 16)  0           ['cbam_3d_2[0][0]',              \n",
      "                                                                  'conv3d_9[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling3d_1 (MaxPooling3D)  (None, 3, 3, 4, 16)  0          ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " cbam_3d_4 (CBAM_3D)            (None, 3, 3, 4, 16)  835         ['max_pooling3d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv3d_11 (Conv3D)             (None, 1, 1, 2, 32)  13856       ['cbam_3d_4[0][0]']              \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 64)           0           ['conv3d_11[0][0]']              \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 3)            195         ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 38,309\n",
      "Trainable params: 38,309\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = layers.Input(shape = (im_height, im_width, im_depth, im_channel))\n",
    "out1 = layers.Conv3D(filters=8, kernel_size=(3,3,3), padding='same',activation='relu',input_shape=(im_height, im_width, im_depth, im_channel))(input_layer)\n",
    "out2 = CBAM_3D(out1.shape[4],4)(out1,out1.shape[1],out1.shape[2],out1.shape[3],out1.shape[4])\n",
    "out2 = layers.Conv3D(filters=8, kernel_size=(3,3,3), padding='same',activation='relu',input_shape=(im_height, im_width, im_depth, im_channel))(out1)\n",
    "out2 = CBAM_3D(out2.shape[4],4)(out2,out2.shape[1],out2.shape[2],out2.shape[3],out2.shape[4])\n",
    "out3 = layers.Conv3D(filters=8, kernel_size=(3,3,3), padding='same',activation='relu',input_shape=(im_height, im_width, im_depth, im_channel))(out2)\n",
    "out4 = layers.Add()([out1, out3])  #Concatenate()\n",
    "out5 = layers.MaxPool3D(pool_size=(2, 2, 4), strides=None, padding='same')(out4)\n",
    "\n",
    "out6 = layers.Conv3D(filters=16, kernel_size=(3,3,3), padding='same',activation='relu',input_shape=(im_height, im_width, im_depth, im_channel))(out5)\n",
    "out6 = CBAM_3D(out6.shape[4],4)(out6,out6.shape[1],out6.shape[2],out6.shape[3],out6.shape[4])\n",
    "out7 = layers.Conv3D(filters=16, kernel_size=(3,3,3), padding='same',activation='relu',input_shape=(im_height, im_width, im_depth, im_channel))(out6)\n",
    "out7 = CBAM_3D(out7.shape[4],4)(out7,out7.shape[1],out7.shape[2],out7.shape[3],out7.shape[4])\n",
    "out8 = layers.Conv3D(filters=16, kernel_size=(3,3,3), padding='same',activation='relu',input_shape=(im_height, im_width, im_depth, im_channel))(out7)\n",
    "out9 = layers.Add()([out6, out8])  #Concatenate()\n",
    "out10 = layers.MaxPool3D(pool_size=(2, 2, 2), strides=None, padding='same')(out9)\n",
    "out10 = CBAM_3D(out10.shape[4],4)(out10,out10.shape[1],out10.shape[2],out10.shape[3],out10.shape[4])\n",
    "out11 = layers.Conv3D(filters=32, kernel_size=(3,3,3), padding='valid',activation='relu',input_shape=(im_height, im_width, im_depth, im_channel))(out10)\n",
    "out12 = layers.Flatten()(out11)\n",
    "output_layer = layers.Dense(3, activation='relu')(out12)\n",
    "FE_model = Model(inputs=input_layer,outputs=output_layer,name='R3CBAM')\n",
    "FE_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRz-xbfnmjOf"
   },
   "source": [
    "### Outlier coarse classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZP81TrgNIj2"
   },
   "outputs": [],
   "source": [
    "# Outlier Coarse Classifier\n",
    "OOD_input = layers.Input(shape = (3))\n",
    "OOD1 = layers.Dense(3, activation='relu')(OOD_input)\n",
    "OOD2 = layers.Dense(6, activation='relu')(OOD1)\n",
    "OOD3 = layers.Dense(3, activation='relu')(OOD2)\n",
    "OOD4 = layers.Dense(5, activation='softmax')(OOD3)\n",
    "OOD_Classifier = Model(inputs=OOD_input,outputs=OOD4,name='OOD_Classifier')\n",
    "OOD_Classifier.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ql1OKEpzUf_B"
   },
   "source": [
    "# Weibull fitting and Calibration by P-OpenMax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BorjEh7XEKNA"
   },
   "outputs": [],
   "source": [
    "# Weibull fitting definition\n",
    "\n",
    "# WEIBULL FIT\n",
    "def calc_supportL2_dists(x, y):\n",
    "  # x : (d,)  # mean\n",
    "  # y : (m,d)  # features of same class\n",
    "  dist = []\n",
    "  for s in y:\n",
    "    dist.append(tf.reduce_mean(tf.math.pow(x - s, 2)).numpy())\n",
    "  return dist\n",
    "\n",
    "def weibull_tailfitting_Low(distance, tailsize=5):  # inside episode - finegrained learning- penalizes more\n",
    "    mr = libmr.MR()   \n",
    "    tailtofit = distance[:tailsize]                              \n",
    "    # tailtofit = sorted(distance)[:tailsize]     \n",
    "    mr.fit_low(tailtofit, len(tailtofit))       \n",
    "    return [mr]\n",
    "\n",
    "def weibull_tailfitting_High(distance, tailsize=5):   # epoch wise - preserves true distribution structure\n",
    "    mr = libmr.MR()                              # learning from samples which are very distant known samples...covers distribution density \n",
    "    tailtofit = distance[:tailsize]\n",
    "    # tailtofit = sorted(distance)[-tailsize:]     # last tailsize distance //  sorted(distance) --> small to big\n",
    "    mr.fit_high(tailtofit, len(tailtofit))      \n",
    "    return [mr]\n",
    "\n",
    "def Build_weibull_modify(z_prototypes,z_proto,class_labels,mode):\n",
    "  Prototypes, S_distances, S_weibull= [], [], [[libmr.MR()] for i in range(16)]\n",
    "  p = 0\n",
    "  #weibull_m = {}\n",
    "  for Proto_categogical in z_prototypes: \n",
    "    supp_categogical = z_proto[p]   # support set features of each class\n",
    "    S_dis = calc_supportL2_dists(Proto_categogical,supp_categogical)              # Pending: Each epsisode updat distance with prev distance difference?\n",
    "    Prototypes.append(Proto_categogical)\n",
    "    S_distances.append(S_dis)\n",
    "    if mode=='episode':\n",
    "      S_weibull[class_labels[p]-1]=weibull_tailfitting_Low(S_dis)         \n",
    "    elif mode == 'epoch':\n",
    "      S_weibull[class_labels[p]-1]=weibull_tailfitting_High(S_dis)\n",
    "    p = p+1\n",
    "    \n",
    "  weibull_model = {}\n",
    "  weibull_model['mean_proto'] = np.array(Prototypes)  # (3, 64)\n",
    "  weibull_model['dist_proto'] = np.array(S_distances) # (3, 5) \n",
    "  weibull_model['model'] = S_weibull\n",
    "  return weibull_model  \n",
    "\n",
    "\n",
    "def Build_weibull(z_prototypes,z_proto,mode):\n",
    "  Prototypes, S_distances, S_weibull= [], [], []\n",
    "  p = 0\n",
    "  #weibull_m = {}\n",
    "  for Proto_categogical in z_prototypes: \n",
    "    supp_categogical = z_proto[p]   # support set features of each class\n",
    "    S_dis = calc_supportL2_dists(Proto_categogical,supp_categogical)              # Pending: Each epsisode updat distance with prev distance difference?\n",
    "    Prototypes.append(Proto_categogical)\n",
    "    S_distances.append(S_dis)\n",
    "    if mode=='episode':\n",
    "      S_weibull.append(weibull_tailfitting_Low(S_dis))         \n",
    "    elif mode == 'epoch':\n",
    "      S_weibull.append(weibull_tailfitting_High(S_dis))\n",
    "    p = p+1\n",
    "    \n",
    "  weibull_model = {}\n",
    "  weibull_model['mean_proto'] = np.array(Prototypes)  # (3, 64)\n",
    "  weibull_model['dist_proto'] = np.array(S_distances) # (3, 5) \n",
    "  weibull_model['model'] = S_weibull\n",
    "  return weibull_model\n",
    "\n",
    "def Calculate_OpenMax(qembed, predictions, weibull_model, NCLASSES):\n",
    "  alpharank = NCLASSES\n",
    "  Open_predictions = []\n",
    "  \n",
    "  for q in range(qembed.shape[0]): \n",
    "    Q_embed = qembed[q]                                                         \n",
    "    prediction = predictions[q].numpy()                                          \n",
    "    AV = qembed[q]                                                               \n",
    "  \n",
    "    ranked_list = prediction.argsort().ravel()[::-1]                            # largest to smallest probability index    \n",
    "    alpha_weights = [((alpharank+1) - i)/float(alpharank) for i in range(1, alpharank+1)]            # [1.0, 0.66, 0.33]    if alpharank = 3\n",
    "    ranked_alpha = np.zeros(NCLASSES)\n",
    "    for i in range(len(alpha_weights)):\n",
    "        ranked_alpha[ranked_list[i]] = alpha_weights[i]                         # Assign weightage at top alpharank pos of ranked_list> 1st pos of ranked_list higest weight # [0.66, 0.33, 1]\n",
    "\n",
    "    openmax_Known, openmax_Unknown = [], []\n",
    "    p=0\n",
    "    for categoryid in range(NCLASSES):\n",
    "      Q_distance = tf.reduce_mean(tf.math.pow(weibull_model['mean_proto'][p] - Q_embed, 2)).numpy()     \n",
    "      wscore = weibull_model['model'][p][0].w_score(Q_distance)                                         \n",
    "      \n",
    "      modified_fc8_score = AV[categoryid] * (1 - wscore*ranked_alpha[categoryid])                       \n",
    "      openmax_Known += [modified_fc8_score]                                                             \n",
    "      openmax_Unknown += [AV[categoryid] - modified_fc8_score]                                          \n",
    "      p=p+1\n",
    "\n",
    "    prob_scores, prob_unknowns,k_scores = [], [], []\n",
    "    for category in range(NCLASSES):\n",
    "      k_scores += [np.exp(openmax_Known[category])]                                                     \n",
    "    total_denominator = np.sum(np.exp(openmax_Known[:])) + np.exp(np.sum(openmax_Unknown[:]))          \n",
    "    prob_scores += [k_scores / total_denominator]                                                       \n",
    "    prob_unknowns += [np.exp(np.sum(openmax_Unknown[:]))/total_denominator]                             \n",
    "    \n",
    "    scores = np.mean(np.asarray(prob_scores), axis=0)                                                   \n",
    "    unknowns = np.mean(np.asarray(prob_unknowns), axis=0)                                               \n",
    "    openmax_scores = scores.tolist() + [unknowns]                                                       \n",
    "    assert len(openmax_scores) == NCLASSES + 1\n",
    "\n",
    "    Open_predictions.append(openmax_scores)                                                            \n",
    "  \n",
    "  Open_predictions = np.array(Open_predictions)   \n",
    "  return Open_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DxlSD8A3OJU9"
   },
   "outputs": [],
   "source": [
    "# Weibull for Coarse Classes Definition\n",
    "\n",
    "def Build_weibull_coarse(z_prototypes,z_proto,class_labels,mode):\n",
    "  Prototypes, S_distances, S_weibull= [], [], [[libmr.MR()] for i in range(4)]\n",
    "  p = 0\n",
    "  #weibull_m = {}\n",
    "  for Proto_categogical in z_prototypes: \n",
    "    supp_categogical = z_proto[p]   # support set features of each class\n",
    "    S_dis = calc_supportL2_dists(Proto_categogical,supp_categogical)              # Pending: Each epsisode updat distance with prev distance difference?\n",
    "    Prototypes.append(Proto_categogical)\n",
    "    S_distances.append(S_dis)\n",
    "    if mode=='episode':\n",
    "      S_weibull[class_labels[p]-1]=weibull_tailfitting_Low(S_dis)         \n",
    "    elif mode == 'epoch':\n",
    "      S_weibull[class_labels[p]-1]=weibull_tailfitting_High(S_dis) \n",
    "    p = p+1\n",
    "    \n",
    "  weibull_model = {}\n",
    "  weibull_model['mean_proto'] = np.array(Prototypes)  # (3, 64)\n",
    "  weibull_model['dist_proto'] = np.array(S_distances) # (3, 5) \n",
    "  weibull_model['model'] = S_weibull\n",
    "  return weibull_model  \n",
    "\n",
    "\n",
    "def Build_weibull_modify(z_prototypes,z_proto,class_labels,mode):\n",
    "  Prototypes, S_distances, S_weibull= [], [], [[libmr.MR()] for i in range(16)]\n",
    "  p = 0\n",
    "  #weibull_m = {}\n",
    "  for Proto_categogical in z_prototypes: \n",
    "    supp_categogical = z_proto[p]   # support set features of each class\n",
    "    S_dis = calc_supportL2_dists(Proto_categogical,supp_categogical)              # Pending: Each epsisode updat distance with prev distance difference?\n",
    "    Prototypes.append(Proto_categogical)\n",
    "    S_distances.append(S_dis)\n",
    "    if mode=='episode':\n",
    "      S_weibull[class_labels[p]-1]=weibull_tailfitting_Low(S_dis)         \n",
    "    elif mode == 'epoch':\n",
    "      S_weibull[class_labels[p]-1]=weibull_tailfitting_High(S_dis)      \n",
    "    p = p+1\n",
    "    \n",
    "  weibull_model = {}\n",
    "  weibull_model['mean_proto'] = np.array(Prototypes)  # (3, 64)\n",
    "  weibull_model['dist_proto'] = np.array(S_distances) # (3, 5) \n",
    "  weibull_model['model'] = S_weibull\n",
    "  return weibull_model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XfMEn8NITjoI"
   },
   "outputs": [],
   "source": [
    "# P-OpenMax Layer\n",
    "\n",
    "def Calculate_OpenMax_modify(qembed, predictions, weibull_model, NCLASSES,class_labels):\n",
    "  alpharank = NCLASSES\n",
    "  Open_predictions = []\n",
    "  \n",
    "  for q in range(qembed.shape[0]): \n",
    "    Q_embed = qembed[q]                                                         \n",
    "    prediction = predictions[q].numpy()                                         \n",
    "    AV = qembed[q]                                                               \n",
    "  \n",
    "    ranked_list = prediction.argsort().ravel()[::-1]                            # largest to smallest probability index     # i.e. [2,0,1]\n",
    "    alpha_weights = [((alpharank+1) - i)/float(alpharank) for i in range(1, alpharank+1)]            # [1.0, 0.66, 0.33]    if alpharank = 3\n",
    "    ranked_alpha = np.zeros(NCLASSES)\n",
    "    for i in range(len(alpha_weights)):\n",
    "        ranked_alpha[ranked_list[i]] = alpha_weights[i]                         # Assign weightage at top alpharank pos of ranked_list> 1st pos of ranked_list higest weight # [0.66, 0.33, 1]\n",
    "\n",
    "    openmax_Known, openmax_Unknown = [], []\n",
    "    p=0\n",
    "    for categoryid in range(NCLASSES):\n",
    "      Q_distance = tf.reduce_mean(tf.math.pow(weibull_model['mean_proto'][p] - Q_embed, 2)).numpy()     \n",
    "      wscore = weibull_model['model'][class_labels[p]-1][0].w_score(Q_distance)                                         \n",
    "      \n",
    "      modified_fc8_score = AV[categoryid] * (1 - wscore*ranked_alpha[categoryid])                       \n",
    "      openmax_Known += [modified_fc8_score]                                                             \n",
    "      openmax_Unknown += [AV[categoryid] - modified_fc8_score]                                          \n",
    "      p=p+1\n",
    "\n",
    "    prob_scores, prob_unknowns,k_scores = [], [], []\n",
    "    for category in range(NCLASSES):\n",
    "      k_scores += [np.exp(openmax_Known[category])]                                                     \n",
    "    total_denominator = np.sum(np.exp(openmax_Known[:])) + np.exp(np.sum(openmax_Unknown[:]))           \n",
    "    prob_scores += [k_scores / total_denominator]                                                       \n",
    "    prob_unknowns += [np.exp(np.sum(openmax_Unknown[:]))/total_denominator]                            \n",
    "    \n",
    "    scores = np.mean(np.asarray(prob_scores), axis=0)                                                   \n",
    "    unknowns = np.mean(np.asarray(prob_unknowns), axis=0)                                               \n",
    "    openmax_scores = scores.tolist() + [unknowns]                                                       \n",
    "    assert len(openmax_scores) == NCLASSES + 1\n",
    "\n",
    "    Open_predictions.append(openmax_scores)                                                             \n",
    "  \n",
    "  Open_predictions = np.array(Open_predictions)   \n",
    "  return Open_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jb_vFZbnLmCP"
   },
   "source": [
    "### Compute distance from Prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_nfkYMHw-Xco"
   },
   "outputs": [],
   "source": [
    "def calc_euclidian_dists(x, y):\n",
    "  # x : (n,d)\n",
    "  # y : (m,d)\n",
    "    n = x.shape[0]\n",
    "    m = y.shape[0]\n",
    "    x = tf.tile(tf.expand_dims(x, 1), [1, m, 1])\n",
    "    y = tf.tile(tf.expand_dims(y, 0), [n, 1, 1])\n",
    "    return tf.reduce_mean(tf.math.pow(x - y, 2), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQhTXdgyMHgP"
   },
   "outputs": [],
   "source": [
    "CQ = 6 #C2\n",
    "CS = 3 #C1\n",
    "N = 15\n",
    "K = 5\n",
    "optim = tf.keras.optimizers.Adam(0.001)    # FE\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "OODoptim = tf.keras.optimizers.Adam(0.001)   # OOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dx3bk3ulLQr"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJLcFCJ8vwTy"
   },
   "outputs": [],
   "source": [
    "%mkdir TIP_fsosr\n",
    "checkpoint_dir = '/content/Fsosr_EVML'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optim,FE_model = FE_model, OOD_Classifier=OOD_Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKIG82OqtL6p"
   },
   "outputs": [],
   "source": [
    "emb_dim = 3\n",
    "\n",
    "def proto_train(ep_class_images,ep_query_images,ep_class_labels,ep_query_labels,support_classes,K,CS,CQ,N):#5,3,6,15     \n",
    "    outlier = 0\n",
    "    sembed = FE_model(ep_class_images)                             # [15, 3]        \n",
    "    qembed = FE_model(ep_query_images)                             # [90, 3]\n",
    "    y_query = np.asarray(np.zeros((len(ep_query_images),CS)),dtype=np.float32)  # (90, 3) \n",
    "    y_true = np.zeros(len(ep_query_labels)) #for storing labels of classes, 0 for unseen; 1,2,3 for the three classes    #(90)\n",
    "    for i in range(len(ep_query_labels)) :\n",
    "      if ep_query_labels[i] in support_classes :\n",
    "            x = support_classes.index(ep_query_labels[i])\n",
    "            y_query[i][x] = 1.                                      # [[0., 0., 1.], [0., 0., 0.], ... (90,3)\n",
    "            y_true[i] = x+1\n",
    "    y_support = np.asarray(np.zeros((len(ep_class_images),CS)),dtype=np.float32)  # (15, 3) \n",
    "    for i in range(len(ep_class_labels)) :\n",
    "      if ep_class_labels[i] in support_classes :\n",
    "            x = support_classes.index(ep_class_labels[i])\n",
    "            y_support[i][x] = 1.                                      # [[0., 0., 1.], [0., 0., 0.], ... (90,3)\n",
    "    z_proto = tf.reshape(sembed,[CS, K, sembed.shape[-1]])           # [3, 5, 3]    \n",
    "    z_prototypes = tf.math.reduce_mean(z_proto, axis=1)              # [3, 3]   \n",
    "\n",
    "    # Calculate closed class loss\n",
    "    dists = calc_euclidian_dists(qembed, z_prototypes)  #sqembedK\n",
    "    log_p_y = tf.nn.log_softmax(-dists,axis=-1)\n",
    "    cec_closed_loss = -tf.reduce_mean((tf.reduce_sum(tf.multiply(y_query, log_p_y), axis=-1)))  #y_sqK     \n",
    "\n",
    "\n",
    "    loss_quad=calc_Quadruplet_dists(z_prototypes, qembed, y_query)\n",
    "\n",
    "    #  Calculate closed class accuracy \n",
    "    predictions_closed = tf.nn.softmax(-dists, axis=-1)\n",
    "    pred_index = tf.argmax(predictions_closed,axis=-1)\n",
    "    correct_pred = 0\n",
    "    for i in range(len(ep_query_labels)) :\n",
    "        if ep_query_labels[i] in support_classes:     #support_labels :\n",
    "            x = support_classes.index(ep_query_labels[i])\n",
    "            if x == pred_index[i] :\n",
    "                correct_pred += 1          \n",
    "    closed_oa = correct_pred/(CS*N)                     #(CQ*N)  # scalar\n",
    "\n",
    "    # Calculate Open class loss\n",
    "    NCLASSES = CS\n",
    "    weibull_model = Build_weibull_modify(z_prototypes,z_proto,support_classes,'episode')    \n",
    "    predictions_Open = Calculate_OpenMax_modify(qembed, predictions_closed, weibull_model, NCLASSES, support_classes) \n",
    "\n",
    "    y_q_open = np.concatenate((y_query,np.zeros((y_query.shape[0],1))),axis=1)  \n",
    "    index = 0\n",
    "    for ind in y_q_open:\n",
    "      if np.sum(ind)==0:\n",
    "        y_q_open[index, NCLASSES] = 1.\n",
    "      index += 1\n",
    "    cec_open_loss = tf.math.abs(-tf.reduce_mean((tf.reduce_sum(tf.multiply(y_q_open, predictions_Open), axis=-1))))\n",
    "    cec_open_loss = tf.cast(cec_open_loss, dtype=tf.float32)\n",
    "    \n",
    "    # Calculate Open class accuracy\n",
    "    pred_open_index = tf.argmax(predictions_Open,axis=-1)\n",
    "    correct_open_index = tf.argmax(y_q_open,axis=-1)\n",
    "\n",
    "\n",
    "    cm = confusion_matrix(correct_open_index,pred_open_index)\n",
    "    FP = cm.sum(axis=0) - np.diag(cm)  \n",
    "    FN = cm.sum(axis=1) - np.diag(cm)\n",
    "    TP = np.diag(cm)\n",
    "    TN = cm.sum() - (FP + FN + TP)\n",
    "    open_oa = (sum(TP)+sum(TN))/(sum(TP)+sum(TN)+sum(FP)+sum(FN))\n",
    "\n",
    "    # calculate Outlier detection accuracy \n",
    "    correct_outlier_index = (correct_open_index==NCLASSES)\n",
    "    pred_outlier_index = (pred_open_index==NCLASSES)\n",
    "    out_equality = tf.math.equal(pred_outlier_index, correct_outlier_index)\n",
    "    outlier_det_acc = tf.math.reduce_mean(tf.cast(out_equality, tf.float32))\n",
    "    outlier_det_acc = outlier_det_acc.numpy()\n",
    "\n",
    "    # Outlier coarse classification\n",
    "    Unkown_classes = list(set(ep_query_labels).difference(set(ep_class_labels)))\n",
    "    U_query, U_gt, ind = [], [], 0\n",
    "    for l in ep_query_labels:\n",
    "      if l in Unkown_classes:\n",
    "        U_query.append(qembed[ind])\n",
    "        U_gt.append(y_coarse[l])\n",
    "      ind = ind+1    \n",
    "    U_query = tf.convert_to_tensor(np.array(U_query))\n",
    "    U_gt = tf.convert_to_tensor(np.array(U_gt))    \n",
    "    for i in range(1):\n",
    "      with tf.GradientTape() as outlier_tape:\n",
    "        Upred = OOD_Classifier(U_query) \n",
    "        outlier_loss = cce(U_gt, Upred)\n",
    "      Ograds = outlier_tape.gradient(outlier_loss, OOD_Classifier.trainable_variables)\n",
    "      OODoptim.apply_gradients(zip(Ograds, OOD_Classifier.trainable_variables))\n",
    "\n",
    "    # coarseoa calculation\n",
    "    u_gt=np.argmax(np.array(U_gt),1)\n",
    "    u_pred=np.argmax(np.array(Upred),1)  \n",
    "    coarse_oa=tf.math.reduce_mean(tf.cast(tf.math.equal(tf.convert_to_tensor(u_gt),tf.convert_to_tensor(u_pred)), tf.float32))\n",
    "    coarse_oa=coarse_oa.numpy()\n",
    "    # Multitask loss  \n",
    "    loss = cec_open_loss + loss_quad + cec_closed_loss + outlier_loss\n",
    "\n",
    "    return loss, closed_oa, outlier_det_acc, open_oa, coarse_oa, z_proto      # return z_proto (class-specific support vector)\n",
    "\n",
    "# Metrics to gather\n",
    "train_loss = tf.metrics.Mean(name='train_loss')\n",
    "train_closedoa = tf.metrics.Mean(name='train_closedoa')\n",
    "train_outlier_acc = tf.metrics.Mean(name='train_outlier_acc')\n",
    "train_openoa = tf.metrics.Mean(name='train_openoa')\n",
    "train_coarse_oa = tf.metrics.Mean(name='train_coarse_oa')\n",
    "\n",
    "def train_step(tsupport_patches,tquery_patches,support_labels,query_labels,support_classes,K,CS,CQ,N):\n",
    "    # Forward & update gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, accuracy, outlier_det_acc, openoa, coarse_oa, z_proto = proto_train(tsupport_patches,tquery_patches,support_labels,query_labels,support_classes,K,CS,CQ,N)\n",
    "    gradients = tape.gradient(loss, FE_model.trainable_variables)\n",
    "    optim.apply_gradients(zip(gradients, FE_model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_closedoa(accuracy)   \n",
    "    train_outlier_acc(outlier_det_acc)\n",
    "    train_openoa(openoa)\n",
    "    train_coarse_oa(coarse_oa)\n",
    "    return z_proto\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir logs/gradient_tape    \n",
    "\n",
    "OCATrain = []\n",
    "for epoch in range(1000):                                       #ToDo\n",
    "    train_loss.reset_states()  \n",
    "    train_closedoa.reset_states()   \n",
    "    train_outlier_acc.reset_states()\n",
    "    train_openoa.reset_states()\n",
    "    train_coarse_oa.reset_states()\n",
    "\n",
    "    Sembed_Dict = {2:[], 3:[], 5:[], 6:[], 8:[], 10:[], 11:[], 12:[], 14:[], 15:[]}\n",
    "    # [2,3,5,6,8,10,11,12,14,15]\n",
    "    for epi in range(10): \n",
    "        tquery_patches, tsupport_patches, query_labels, support_labels, support_classes = new_episode(patches_class_ip,K,N,CS,CQ,train_class_labels)   \n",
    "        z_p = train_step(tsupport_patches,tquery_patches,support_labels,query_labels,support_classes,K,CS,CQ,N)   # z_p = z_proto   #[3, 5, 3]\n",
    "        #Accumulator: store return z_proto per support_classes per episode\n",
    "        ind = 0\n",
    "        for sl in support_classes:\n",
    "          Sembed_Dict[sl].extend(np.array(z_p[ind]))\n",
    "          ind +=1\n",
    "\n",
    "        \n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('closedoa', train_closedoa.result(), step=epoch)\n",
    "        tf.summary.scalar('outlier_det_acc',train_outlier_acc.result(), step=epoch)\n",
    "        tf.summary.scalar('openoa',train_openoa.result(), step=epoch)\n",
    "\n",
    "    template = 'Epoch {}, Train Loss: {:.2f}, closed OA: {:.2f}, Open OA: {:.2f}, Outlier_Det. Acc: {:.2f}, Outlier course Acc: {:.2f}'\n",
    "    print(template.format(epoch+1,train_loss.result(),train_closedoa.result()*100,train_openoa.result()*100,train_outlier_acc.result()*100,train_coarse_oa.result()*100))\n",
    "    OCATrain.append(train_coarse_oa.result())\n",
    " \n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bvzcFUWUNZIm"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/gradient_tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arIG1u87BZq9"
   },
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzvKxHabyVJQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_patches_class = [patches_class_ip[i] for i in train_class_indices]        #(10)\n",
    "test_patches_class = [patches_class_ip[i] for i in test_class_indices]        #(6) \n",
    "test_support_labels = [1,4,7]\n",
    "ft_labels = [1,2,3,4,5,6,7,8,10,11,12,14,15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RmDTAp09BWs5"
   },
   "outputs": [],
   "source": [
    "tune_set_5 = [[] for i in range(16)]\n",
    "for j in range(1,17) :\n",
    "  if j in train_class_labels :\n",
    "    tune_set_5[j-1] = patches_class_ip[j-1] \n",
    "  elif j in test_support_labels :\n",
    "    tune_set_5[j-1] = patches_class_ip[j-1][:5,:,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JS1TPJ-5hWbD"
   },
   "outputs": [],
   "source": [
    "print(len(train_patches_class))\n",
    "print(train_patches_class[1].shape)\n",
    "print(len(test_patches_class))\n",
    "print(train_class_labels)\n",
    "print(test_class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvON3Mk8HZQg"
   },
   "outputs": [],
   "source": [
    "def tune_episode(patches_list,NS,NQ,CS,CQ,class_labels) :  # NS 5,NQ 15,CS 3,CQ 6\n",
    "    selected_classes = list(np.random.choice(class_labels,CQ,replace=False))  # Randomly choice 6 Query Classes\n",
    "    support_classes = list(np.random.choice(selected_classes,CS,replace=False))  # Randomly choice 3 Support Classes from Q\n",
    "    \n",
    "    tquery_patches,tsupport_patches = [],[]\n",
    "    query_labels,support_labels = [],[]\n",
    "    \n",
    "    for x in support_classes :      #3\n",
    "        sran_indices = np.random.choice(patches_list[x-1].shape[0],NS,replace=False) # K=5 img nos from 20 per class for Support\n",
    "        support_patches = patches_list[x-1][sran_indices,:,:,:,:]  # for x class those 5 nos\n",
    "        tsupport_patches.extend(support_patches)               # 3 class * 5 patch per class = 15 patches\n",
    "        for i in range(NS) :\n",
    "            support_labels.append(x)                           # 3 class *5 nos = 15 nos\n",
    "        \n",
    "    for x in selected_classes :     #6\n",
    "        qran_indices = np.random.choice(patches_list[x-1].shape[0],NQ,replace=False) # NQ=15 img nos from 20 per class for Query\n",
    "        query_patches = patches_list[x-1][qran_indices,:,:,:,:]    # for x class those 15 nos\n",
    "        tquery_patches.extend(query_patches)                   # 6 class * 15 patch per class = 90 patches\n",
    "        for i in range(NQ) :\n",
    "            query_labels.append(x)                             # 6 class * 15 nos = 90 nos\n",
    "    \n",
    "    temp1 = list(zip(tquery_patches, query_labels)) \n",
    "    random.shuffle(temp1)        # By Doing Shuffling, Support, Query Same class combination got mismatched - mitigated by support index\n",
    "    tquery_patches, query_labels = zip(*temp1)\n",
    "    \n",
    "    tquery_patches = tf.convert_to_tensor(np.reshape(np.asarray(tquery_patches),(CQ*NQ,11,11,30,1)),dtype=tf.float32)\n",
    "    tsupport_patches = tf.convert_to_tensor(np.reshape(np.asarray(tsupport_patches),(CS*NS,11,11,30,1)),dtype=tf.float32)\n",
    "    return tquery_patches, tsupport_patches, query_labels, support_labels, support_classes    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjRuk78-GmEk"
   },
   "outputs": [],
   "source": [
    "%mkdir Tune_TIP\n",
    "checkpoint_dir_tune = '/content/Fsosr_EVML_Tune'         # To change\n",
    "checkpoint_prefix_tune = os.path.join(checkpoint_dir_tune, \"ckpt\")\n",
    "checkpoint_tune = tf.train.Checkpoint(optimizer=optim,FE_model = FE_model, OOD_Classifier=OOD_Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WCQaGBAPB7-l"
   },
   "outputs": [],
   "source": [
    "emb_dim = 3\n",
    "tK = 1\n",
    "tN = 4\n",
    "\n",
    "def proto_tune(ep_class_images,ep_query_images,ep_class_labels,ep_query_labels,support_classes,tK,CS,CQ,tN):   \n",
    "    outlier = 0\n",
    "    sembed = FE_model(ep_class_images)                             # [15, 3]        \n",
    "    qembed = FE_model(ep_query_images)                             # [90, 3]\n",
    "    y_query = np.asarray(np.zeros((len(ep_query_images),CS)),dtype=np.float32)  # (90, 3) \n",
    "    y_true = np.zeros(len(ep_query_labels)) #for storing labels of classes, 0 for unseen; 1,2,3 for the three classes    #(90)\n",
    "    for i in range(len(ep_query_labels)) :\n",
    "      if ep_query_labels[i] in support_classes :\n",
    "            x = support_classes.index(ep_query_labels[i])\n",
    "            y_query[i][x] = 1.                                      # [[0., 0., 1.], [0., 0., 0.], ... (90,3)\n",
    "            y_true[i] = x+1\n",
    "    y_support = np.asarray(np.zeros((len(ep_class_images),CS)),dtype=np.float32)  # (15, 3) \n",
    "    for i in range(len(ep_class_labels)) :\n",
    "      if ep_class_labels[i] in support_classes :\n",
    "            x = support_classes.index(ep_class_labels[i])\n",
    "            y_support[i][x] = 1.                                      # [[0., 0., 1.], [0., 0., 0.], ... (90,3)\n",
    "    z_proto = tf.reshape(sembed,[CS, tK, sembed.shape[-1]])           # [3, 5, 3]    \n",
    "    z_prototypes = tf.math.reduce_mean(z_proto, axis=1)              # [3, 3]   \n",
    "\n",
    "    # Calculate closed class loss\n",
    "    dists = calc_euclidian_dists(qembed, z_prototypes)  \n",
    "    log_p_y = tf.nn.log_softmax(-dists,axis=-1)\n",
    "    cec_closed_loss = -tf.reduce_mean((tf.reduce_sum(tf.multiply(y_query, log_p_y), axis=-1)))  \n",
    "\n",
    "    loss_quad=calc_Quadruplet_dists(z_prototypes, qembed, y_query)\n",
    "\n",
    "    #  Calculate closed class accuracy \n",
    "    predictions_closed = tf.nn.softmax(-dists, axis=-1)\n",
    "    pred_index = tf.argmax(predictions_closed,axis=-1)\n",
    "    correct_pred = 0\n",
    "    for i in range(len(ep_query_labels)) :\n",
    "        if ep_query_labels[i] in support_classes:     #support_labels :\n",
    "            x = support_classes.index(ep_query_labels[i])\n",
    "            if x == pred_index[i] :\n",
    "                correct_pred += 1          \n",
    "    closed_oa = correct_pred/(CS*tN)                     #(CQ*N)  # scalar\n",
    "\n",
    "    # Calculate Open class loss\n",
    "    NCLASSES = CS\n",
    "    weibull_model = Build_weibull_modify(z_prototypes,z_proto,support_classes,'episode')    \n",
    "    predictions_Open = Calculate_OpenMax_modify(qembed, predictions_closed, weibull_model, NCLASSES, support_classes) \n",
    "\n",
    "    y_q_open = np.concatenate((y_query,np.zeros((y_query.shape[0],1))),axis=1)   \n",
    "    index = 0\n",
    "    for ind in y_q_open:\n",
    "      if np.sum(ind)==0:\n",
    "        y_q_open[index, NCLASSES] = 1.\n",
    "      index += 1\n",
    "    cec_open_loss = tf.math.abs(-tf.reduce_mean((tf.reduce_sum(tf.multiply(y_q_open, predictions_Open), axis=-1))))\n",
    "    cec_open_loss = tf.cast(cec_open_loss, dtype=tf.float32)\n",
    "    \n",
    "    # Calculate Open class accuracy\n",
    "    pred_open_index = tf.argmax(predictions_Open,axis=-1)\n",
    "    correct_open_index = tf.argmax(y_q_open,axis=-1)\n",
    "\n",
    "    cm = confusion_matrix(correct_open_index,pred_open_index)\n",
    "    FP = cm.sum(axis=0) - np.diag(cm)  \n",
    "    FN = cm.sum(axis=1) - np.diag(cm)\n",
    "    TP = np.diag(cm)\n",
    "    TN = cm.sum() - (FP + FN + TP)\n",
    "    open_oa = (sum(TP)+sum(TN))/(sum(TP)+sum(TN)+sum(FP)+sum(FN))\n",
    "\n",
    "    # calculate Outlier detection accuracy \n",
    "    correct_outlier_index = (correct_open_index==NCLASSES)\n",
    "    pred_outlier_index = (pred_open_index==NCLASSES)\n",
    "    out_equality = tf.math.equal(pred_outlier_index, correct_outlier_index)\n",
    "    outlier_det_acc = tf.math.reduce_mean(tf.cast(out_equality, tf.float32))\n",
    "    outlier_det_acc = outlier_det_acc.numpy()\n",
    "\n",
    "    # Outlier coarse classification\n",
    "    Unkown_classes = list(set(ep_query_labels).difference(set(ep_class_labels)))\n",
    "    U_query, U_gt, ind = [], [], 0\n",
    "    for l in ep_query_labels:\n",
    "      if l in Unkown_classes:\n",
    "        U_query.append(qembed[ind])\n",
    "        U_gt.append(y_coarse[l])\n",
    "      ind = ind+1    \n",
    "    U_query = tf.convert_to_tensor(np.array(U_query))\n",
    "    U_gt = tf.convert_to_tensor(np.array(U_gt))    \n",
    "    with tf.GradientTape() as outlier_tape:\n",
    "      Upred = OOD_Classifier(U_query) \n",
    "      outlier_loss = cce(U_gt, Upred)\n",
    "    Ograds = outlier_tape.gradient(outlier_loss, OOD_Classifier.trainable_variables)\n",
    "    OODoptim.apply_gradients(zip(Ograds, OOD_Classifier.trainable_variables))\n",
    "\n",
    "    # coarseoa calculation\n",
    "    u_gt=np.argmax(np.array(U_gt),1)\n",
    "    u_pred=np.argmax(np.array(Upred),1)  \n",
    "    coarse_oa=tf.math.reduce_mean(tf.cast(tf.math.equal(tf.convert_to_tensor(u_gt),tf.convert_to_tensor(u_pred)), tf.float32))\n",
    "    coarse_oa=coarse_oa.numpy()\n",
    "    # Multitask loss  \n",
    "\n",
    "    loss = cec_open_loss + loss_quad + cec_closed_loss + outlier_loss\n",
    "    return loss, closed_oa, outlier_det_acc, open_oa, coarse_oa, z_proto      # return z_proto (class-specific support vector)\n",
    "\n",
    "# Metrics to gather\n",
    "tune_loss = tf.metrics.Mean(name='train_loss')\n",
    "tune_closedoa = tf.metrics.Mean(name='train_closedoa')\n",
    "tune_outlier_acc = tf.metrics.Mean(name='train_outlier_acc')\n",
    "tune_openoa = tf.metrics.Mean(name='train_openoa')\n",
    "tune_coarse_oa = tf.metrics.Mean(name='tune_coarse_oa')\n",
    "\n",
    "def tune_step(tsupport_patches,tquery_patches,support_labels,query_labels,support_classes,tK,CS,CQ,tN):\n",
    "    # Forward & update gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, accuracy, outlier_det_acc, openoa, coarse_oa, z_proto = proto_train(tsupport_patches,tquery_patches,support_labels,query_labels,support_classes,tK,CS,CQ,tN)\n",
    "    gradients = tape.gradient(loss, FE_model.trainable_variables)\n",
    "    optim.apply_gradients(zip(gradients, FE_model.trainable_variables))\n",
    "    tune_loss(loss)\n",
    "    tune_closedoa(accuracy)   \n",
    "    tune_outlier_acc(outlier_det_acc)\n",
    "    tune_openoa(openoa)\n",
    "    tune_coarse_oa(coarse_oa)\n",
    "    return z_proto\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tune_log_dir = 'logs/gradient_tape/' + current_time + '/tune'\n",
    "tune_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir logs/gradient_tape    \n",
    "\n",
    "OCATune = []\n",
    "for epoch in range(1000):                                       #ToDo\n",
    "    tune_loss.reset_states()  \n",
    "    tune_closedoa.reset_states()   \n",
    "    tune_outlier_acc.reset_states()\n",
    "    tune_openoa.reset_states()\n",
    "    tune_coarse_oa.reset_states()\n",
    "\n",
    "    Sembed_Dict = {1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[], 12:[], 10:[], 15:[], 16:[]}\n",
    "\n",
    "    for epi in range(10): \n",
    "        tquery_patches, tsupport_patches, query_labels, support_labels, support_classes = tune_episode(tune_set_5,1,4,3,6,ft_labels)   \n",
    "        z_p = tune_step(tsupport_patches,tquery_patches,support_labels,query_labels,support_classes,tK,CS,CQ,tN)   # z_p = z_proto   #[3, 5, 3]\n",
    "        #Accumulator: store return z_proto per support_classes per episode\n",
    "\n",
    "        \n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', tune_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('closedoa', tune_closedoa.result(), step=epoch)\n",
    "        tf.summary.scalar('outlier_det_acc',tune_outlier_acc.result(), step=epoch)\n",
    "        tf.summary.scalar('openoa',tune_openoa.result(), step=epoch)\n",
    "\n",
    "    template = 'Epoch {}, Train Loss: {:.2f}, closed OA: {:.2f}, Open OA: {:.2f}, Outlier_Det. Acc: {:.2f}, Outlier course Acc: {:.2f}'\n",
    "    print(template.format(epoch+1,tune_loss.result(),tune_closedoa.result()*100,tune_openoa.result()*100,tune_outlier_acc.result()*100,tune_coarse_oa.result()*100))\n",
    "    OCATune.append(tune_coarse_oa.result())\n",
    " \n",
    "    checkpoint_tune.save(file_prefix = checkpoint_prefix_tune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4L9dJyJweQz"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yLxu3c0WSLKV"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_patches_class = [patches_class_ip[i] for i in train_class_indices]        #(10)\n",
    "test_patches_class = [patches_class_ip[i] for i in test_class_indices]        #(6) \n",
    "test_support_labels = [1,4,7]\n",
    "ft_labels = [1,2,3,4,5,6,7,8,10,11,12,14,15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v2qhNTd6B540"
   },
   "outputs": [],
   "source": [
    "def proto_test(ep_class_images,ep_query_images,ep_class_labels,ep_query_labels,support_classes,K,CS,CQ,N):#5,3,6,15     \n",
    "    outlier = 0\n",
    "    sembed = FE_model(ep_class_images)                             # [15, 64]        \n",
    "    qembed = FE_model(ep_query_images)                             # [90, 64]\n",
    "    y_query = np.asarray(np.zeros((len(ep_query_images),CS)),dtype=np.float32)  # (90, 3) \n",
    "    y_true = np.zeros(len(ep_query_labels)) #for storing labels of classes, 0 for unseen; 1,2,3 for the three classes\n",
    "    y_auc = np.zeros((len(ep_query_labels))) #for storing labels, 1 for seen, and 0 for unseen\n",
    "    for i in range(len(ep_query_labels)) :\n",
    "      if ep_query_labels[i] in support_classes :\n",
    "            x = support_classes.index(ep_query_labels[i])\n",
    "            y_query[i][x] = 1.                                      # [[0., 0., 1.], [0., 0., 0.], ... (90,3)\n",
    "            y_true[i] = x+1\n",
    "            y_auc[i] = 1\n",
    "    y_support = np.asarray(np.zeros((len(ep_class_images),CS)),dtype=np.float32)  # (15, 3) \n",
    "    for i in range(len(ep_class_labels)) :\n",
    "      if ep_class_labels[i] in support_classes :\n",
    "            x = support_classes.index(ep_class_labels[i])\n",
    "            y_support[i][x] = 1.                                      # [[0., 0., 1.], [0., 0., 0.], ... (90,3)\n",
    "    z_proto = tf.reshape(sembed,[CS, K, sembed.shape[-1]])           # [3, 5, 64]\n",
    "    z_prototypes = tf.math.reduce_mean(z_proto, axis=1)        # [3, 64]   \n",
    "    \n",
    "    \n",
    "    # Calculate closed class loss\n",
    "    dists = calc_euclidian_dists(qembed, z_prototypes) \n",
    "    log_p_y = tf.nn.log_softmax(-dists,axis=-1)\n",
    "    cec_closed_loss = -tf.reduce_mean((tf.reduce_sum(tf.multiply(y_query, log_p_y), axis=-1))) \n",
    "\n",
    "    #  Calculate closed class accuracy \n",
    "    predictions_closed  = tf.nn.softmax(-dists, axis=-1)\n",
    "    pred_index = tf.argmax(predictions_closed ,axis=-1)\n",
    "    correct_pred = 0\n",
    "    for i in range(len(ep_query_labels)) :\n",
    "        if ep_query_labels[i] in support_classes:     #support_labels :\n",
    "            x = support_classes.index(ep_query_labels[i])\n",
    "            if x == pred_index[i] :\n",
    "                correct_pred += 1          \n",
    "    closed_oa = correct_pred/(CS*N)                     #(CQ*N)  # scalar\n",
    "\n",
    "    # Calculate Open class loss\n",
    "    NCLASSES = CS\n",
    "    weibull_model = Build_weibull_modify(z_prototypes,z_proto,support_classes,'episode')               \n",
    "    predictions_Open = Calculate_OpenMax_modify(qembed, predictions_closed, weibull_model, NCLASSES, support_classes)\n",
    "\n",
    "    y_q_open = np.concatenate((y_query,np.zeros((y_query.shape[0],1))),axis=1)    \n",
    "    index = 0\n",
    "    for ind in y_q_open:\n",
    "      if np.sum(ind)==0:\n",
    "        y_q_open[index, NCLASSES] = 1.\n",
    "      index += 1\n",
    "    cec_open_loss = tf.math.abs(-tf.reduce_mean((tf.reduce_sum(tf.multiply(y_q_open, predictions_Open), axis=-1))))\n",
    "    cec_open_loss = tf.cast(cec_open_loss, dtype=tf.float32)\n",
    "    \n",
    "    # Calculate Open class accuracy\n",
    "    pred_open_index = tf.argmax(predictions_Open,axis=-1)\n",
    "    correct_open_index = tf.argmax(y_q_open,axis=-1)\n",
    "\n",
    "    cm = confusion_matrix(correct_open_index,pred_open_index)\n",
    "    FP = cm.sum(axis=0) - np.diag(cm)  \n",
    "    FN = cm.sum(axis=1) - np.diag(cm)\n",
    "    TP = np.diag(cm)\n",
    "    TN = cm.sum() - (FP + FN + TP)\n",
    "    open_oa = (sum(TP)+sum(TN))/(sum(TP)+sum(TN)+sum(FP)+sum(FN))\n",
    "    \n",
    "\n",
    "    # calculate Outlier detection accuracy \n",
    "    correct_outlier_index = (correct_open_index==NCLASSES)\n",
    "    pred_outlier_index = (pred_open_index==NCLASSES)\n",
    "    out_equality = tf.math.equal(pred_outlier_index, correct_outlier_index)\n",
    "    outlier_det_acc = tf.math.reduce_mean(tf.cast(out_equality, tf.float32))\n",
    "    outlier_det_acc = outlier_det_acc.numpy()\n",
    "\n",
    "    # outlier coarse accuracy\n",
    "    coarse_Open_pred = []\n",
    "    OOD_GT = []\n",
    "    for q in range(len(predictions_Open)):\n",
    "      coarse_Open_pred.append(np.argmax(predictions_Open[q]))  \n",
    "    coarse_OOD_ind = [i for i, x in enumerate(coarse_Open_pred) if x==3]   # ouliers will be always at 3\n",
    "\n",
    "\n",
    "    # compute AUROC\n",
    "    y_score = np.zeros((len(ep_query_labels)))\n",
    "    for i in range(len(ep_query_labels)) :\n",
    "      y_score[i] = predictions_Open[i,3]    # outlier prob > openmax last class\n",
    "    auc = sklearn.metrics.roc_auc_score(y_auc, y_score)\n",
    "    \n",
    "    \n",
    "    Unkown_classes = list(set(ep_query_labels).difference(set(ep_class_labels)))\n",
    "    U_query, U_gt, ind = [], [], 0\n",
    "    for l in ep_query_labels:\n",
    "      if l in Unkown_classes:\n",
    "        U_query.append(qembed[ind])\n",
    "        U_gt.append(y_coarse[l])\n",
    "      ind = ind+1    \n",
    "    U_query = tf.convert_to_tensor(np.array(U_query))\n",
    "    U_gt = tf.convert_to_tensor(np.array(U_gt))    \n",
    "    Upred = OOD_Classifier(U_query) \n",
    "\n",
    "    # coarseoa calculation\n",
    "    u_gt=np.argmax(np.array(U_gt),1)\n",
    "    u_pred=np.argmax(np.array(Upred),1)  \n",
    "    coarse_oa=tf.math.reduce_mean(tf.cast(tf.math.equal(tf.convert_to_tensor(u_gt),tf.convert_to_tensor(u_pred)), tf.float32))\n",
    "    coarse_oa=coarse_oa.numpy()\n",
    "\n",
    "    # Multitask loss\n",
    "    loss = cec_closed_loss +cec_open_loss\n",
    "    #print(cec_closed_loss,cec_open_loss,loss)\n",
    "    return loss, closed_oa, outlier_det_acc, open_oa, coarse_oa, auc      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBuKCukctlvN"
   },
   "outputs": [],
   "source": [
    "\n",
    "def test_episode(patches_list,NS,NQ,CS,CQ) :  # NS 5,NQ 15,CS 3,CQ 6\n",
    "    selected_classes = test_class_labels  # 6 Query Classes\n",
    "    support_classes = test_support_labels  # 3 Support Classes from Q\n",
    "    \n",
    "    tquery_patches,tsupport_patches = [],[]\n",
    "    query_labels,support_labels = [],[]\n",
    "    \n",
    "    for x in support_classes :      #3\n",
    "        sran_indices = np.random.choice(patches_list[x-1].shape[0],NS,replace=False) # K=5 img nos from 20 per class for Support\n",
    "        support_patches = patches_list[x-1][sran_indices,:,:,:,:]  # for x class those 5 nos\n",
    "        tsupport_patches.extend(support_patches)               # 3 class * 5 patch per class = 15 patches\n",
    "        for i in range(NS) :\n",
    "            support_labels.append(x)                           # 3 class *5 nos = 15 nos\n",
    "        \n",
    "    for x in selected_classes :     #6\n",
    "        qran_indices = np.random.choice(patches_list[x-1].shape[0],NQ,replace=False) # NQ=15 img nos from 20 per class for Query\n",
    "        query_patches = patches_list[x-1][qran_indices,:,:,:,:]    # for x class those 15 nos\n",
    "        tquery_patches.extend(query_patches)                   # 6 class * 15 patch per class = 90 patches\n",
    "        for i in range(NQ) :\n",
    "            query_labels.append(x)                             # 6 class * 15 nos = 90 nos\n",
    "    \n",
    "    temp1 = list(zip(tquery_patches, query_labels)) \n",
    "    random.shuffle(temp1)        # By Doing Shuffling, Support, Query Same class combination got mismatched - mitigated by support index\n",
    "    tquery_patches, query_labels = zip(*temp1)\n",
    "    \n",
    "    tquery_patches = tf.convert_to_tensor(np.reshape(np.asarray(tquery_patches),(CQ*NQ,11,11,30,1)),dtype=tf.float32)\n",
    "    tsupport_patches = tf.convert_to_tensor(np.reshape(np.asarray(tsupport_patches),(CS*NS,11,11,30,1)),dtype=tf.float32)\n",
    "    return tquery_patches, tsupport_patches, query_labels, support_labels, support_classes    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aynFLEaXB543"
   },
   "outputs": [],
   "source": [
    "total_acc = 0 \n",
    "total_open_oa = 0\n",
    "total_outlier_acc = 0 \n",
    "total_outlier_Course_acc = 0 \n",
    "total_auroc = 0\n",
    "\n",
    "tepochs = 2\n",
    "K = 5\n",
    "N = 15\n",
    "for i in range(tepochs) :\n",
    "  tquery_patches, tsupport_patches, query_labels, support_labels, support_classes = test_episode(patches_class_ip,5,15,3,6)   \n",
    "  loss, closed_oa, outlier_det_acc, open_oa, OOD_course_acc, auc = proto_test(tsupport_patches,tquery_patches,support_labels,query_labels,support_classes,5,3,6,15)\n",
    "  total_acc = total_acc + closed_oa\n",
    "  total_open_oa = total_open_oa + open_oa\n",
    "  total_outlier_acc = total_outlier_acc + outlier_det_acc\n",
    "  total_outlier_Course_acc = total_outlier_Course_acc + OOD_course_acc\n",
    "  total_auroc = total_auroc + auc\n",
    "print('closed accuracy',total_acc*100/tepochs)\n",
    "print('open oa',total_open_oa*100/tepochs)\n",
    "print('Outlier detection accuracy', (total_outlier_acc*100/tepochs))\n",
    "print('Outlier coarse accuracy',total_outlier_Course_acc*100/tepochs)\n",
    "print('AUROC',total_auroc*100/tepochs)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "l-zd2ipd7MSI"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
